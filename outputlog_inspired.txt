python run.py

{'max_c_length': 512, 'max_r_length': 512, 'batch_size': 32, 'max_count': 20, 'use_cpu': True, 'load_dict': None, 'learningrate': 0.001, 'optimizer': 'adam', 'momentum': 0, 'is_finetune': False, 'embedding_type': 'random', 'epoch': 1, 'gpu': '0,1', 'gradient_clip': 0.1, 'embedding_size': 300, 'n_heads': 2, 'n_layers': 2, 'ffn_size': 300, 'dropout': 0.1, 'attention_dropout': 0.0, 'relu_dropout': 0.1, 'learn_positional_embeddings': False, 'embeddings_scale': True, 'n_entity': 64368, 'n_relation': 214, 'n_concept': 29308, 'n_con_relation': 48, 'dim': 128, 'n_hop': 2, 'kge_weight': 1, 'l2_weight': 2.5e-06, 'n_memory': 32, 'item_update_mode': '0,1', 'using_all_hops': True, 'num_bases': 8}
160it [00:00, 10221.28it/s]
160it [00:00, 5121.52it/s]
160it [00:00, 330.02it/s]
160it [00:00, ?it/s]
C:\Users\thami\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
365028 46
  0%|                                                                                                                              | 0/403 [00:00<?, ?it/s]C:\Users\thami\Documents\GitHub\KGSF---movie-recommender-system\models\graph.py:137: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  attention = F.softmax(e)
C:\Users\thami\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
info db loss is 169.698639
 12%|██████████████▎                                                                                                    | 50/403 [38:14<3:58:58, 40.62s/it]info db loss is 7.438884
 25%|███████████████████████████▊                                                                                    | 100/403 [1:09:58<3:20:10, 39.64s/it]info db loss is 0.433145
 37%|█████████████████████████████████████████▋                                                                      | 150/403 [1:42:07<2:38:09, 37.51s/it]info db loss is 0.242626
 50%|███████████████████████████████████████████████████████▌                                                        | 200/403 [2:13:24<2:06:14, 37.31s/it]info db loss is 0.137486
 62%|█████████████████████████████████████████████████████████████████████▍                                          | 250/403 [2:44:39<1:36:54, 38.00s/it]info db loss is 0.030937
 74%|██████████████████████████
  87%|█████████████████████████████████████████████████████████████████████████████████████████████████▎              | 350/403 [4:05:25<1:00:47, 68.82s/it]info db loss is 0.027664
 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 400/403 [5:02:22<03:24, 68.20s/it]info db loss is 0.012783
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 403/403 [5:05:48<00:00, 45.53s/it]
  0%|                                                                                                                              | 0/403 [00:00<?, ?it/s]info db loss is 0.013526
 12%|██████████████▎                                                                                                    | 50/403 [36:12<3:29:55, 35.68s/it]info db loss is 0.033525
 25%|███████████████████████████▊                                                                                    | 100/403 [1:05:39<2:59:11, 35.48s/it]info db loss is 0.189000
 37%|█████████████████████████████████████████▋                                                                      | 150/403 [1:35:14<2:28:21, 35.18s/it]info db loss is 0.075145
 50%|███████████████████████████████████████████████████████▌                                                        | 200/403 [2:04:40<1:59:31, 35.33s/it]info db loss is 0.023338
 62%|█████████████████████████████████████████████████████████████████████▍                                          | 250/403 [2:34:15<1:30:43, 35.58s/it]info db loss is 0.007580
 74%|███████████████████████████████████████████████████████████████████████████████████▎                            | 300/403 [3:03:49<1:01:00, 35.54s/it]info db loss is 0.006183
 87%|███████████████████████████████████████████████████████████████████████████████████████████████████               | 350/403 [3:33:23<31:17, 35.42s/it]info db loss is 0.006299
 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 400/403 [4:02:59<01:46, 35.38s/it]info db loss is 0.005939
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 403/403 [4:04:43<00:00, 36.44s/it]
  0%|                                                                                                                              | 0/403 [00:00<?, ?it/s]info db loss is 0.005798
 12%|██████████████▎                                                                                                    | 50/403 [29:23<3:28:44, 35.48s/it]info db loss is 0.005741
 25%|███████████████████████████▊                                                                                    | 100/403 [1:01:16<3:07:34, 37.14s/it]info db loss is 0.005593
 37%|█████████████████████████████████████████▋                                                                      | 150/403 [1:31:54<2:32:42, 36.21s/it]info db loss is 0.005581
 50%|███████████████████████████████████████████████████████▌                                                        | 200/403 [2:02:33<2:04:46, 36.88s/it]info db loss is 0.005493
 62%|█████████████████████████████████████████████████████████████████████▍                                          | 250/403 [2:32:49<1:31:18, 35.81s/it]info db loss is 0.005391
 74%|███████████████████████████████████████████████████████████████████████████████████▎                            | 300/403 [3:04:30<1:03:43, 37.12s/it]info db loss is 0.005513
 87%|███████████████████████████████████████████████████████████████████████████████████████████████████               | 350/403 [3:34:56<31:09, 35.27s/it]info db loss is 0.005481
 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 400/403 [4:04:16<01:45, 35.19s/it]info db loss is 0.005514
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 403/403 [4:05:56<00:00, 36.62s/it]
masked loss pre-trained
  0%|                                                                                                                              | 0/403 [00:00<?, ?it/s]rec loss is 354.302643 
  info db loss is 0.008332
 12%|██████████████▎                                                                                                    | 50/403 [29:38<3:29:27, 35.60s/it]rec loss is 160.023590 
 info db loss is 15.573775
 25%|████████████████████████████▎                                                                                     | 100/403 [59:18<2:58:48, 35.41s/it]rec loss is 37.734940 
 info db loss is 18.963320
 37%|█████████████████████████████████████████▋                                                                      | 150/403 [1:28:55<2:31:04, 35.83s/it]rec loss is 23.085400 
 info db loss is 6.310967
 50%|███████████████████████████████████████████████████████▌                                                        | 200/403 [1:59:36<2:26:57, 43.44s/it]rec loss is 16.097946 
 info db loss is 5.818381
 62%|█████████████████████████████████████████████████████████████████████▍                                          | 250/403 [2:32:48<1:30:29, 35.49s/it]rec loss is 15.423010 
 info db loss is 4.023755
 74%|███████████████████████████████████████████████████████████████████████████████████▎                            | 300/403 [3:02:36<1:00:36, 35.31s/it]rec loss is 9.462970 
 info db loss is 2.198131
 87%|███████████████████████████████████████████████████████████████████████████████████████████████████               | 350/403 [3:32:33<33:33, 37.99s/it]rec loss is 9.093556 
 info db loss is 3.424098
 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 400/403 [4:03:13<01:53, 37.81s/it]rec loss is 7.505881 
 info db loss is 3.122791
 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 403/403 [4:05:00<00:00, 36.48s/it]
40it [00:00, 2560.63it/s]
40it [00:00, ?it/s]
40it [00:00, 182.90it/s]
40it [00:00, ?it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26/26 [02:26<00:00,  5.64s/it]
{'recall@1': 0.11341463414634147, 'recall@10': 0.16707317073170733, 'recall@50': 0.23902439024390243, 'loss': tensor(14.8840), 'gate': 0.0, 'count': 1.0, 'gate_count': 0.0}
recommendation model saved once------------------------------------------------
80it [00:00, ?it/s]
80it [00:00, 5122.19it/s]
80it [00:00, 269.54it/s]
80it [00:00, ?it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 102/102 [10:16<00:00,  6.04s/it]
{'recall@1': 0.06882716049382716, 'recall@10': 0.13549382716049382, 'recall@50': 0.17901234567901234, 'loss': tensor(16.5167), 'gate': 0.0, 'count': 1.0, 'gate_count': 0.0}
80it [00:00, ?it/s]
80it [00:00, 5106.99it/s]
80it [00:00, 320.13it/s]
80it [00:00, ?it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 102/102 [10:21<00:00,  6.09s/it]
{'recall@1': 0.06882716049382716, 'recall@10': 0.13549382716049382, 'recall@50': 0.17901234567901234, 'loss': tensor(16.5167), 'gate': 0.0, 'count': 1.0, 'gate_count': 0.0}


python run.py --is_finetune True

{'max_c_length': 512, 'max_r_length': 512, 'batch_size': 32, 'max_count': 20, 'use_cpu': True, 'load_dict': None, 'learningrate': 0.001, 'optimizer': 'adam', 'momentum': 0, 'is_finetune': True, 'embedding_type': 'random', 'epoch': 1, 'gpu': '0,1', 'gradient_clip': 0.1, 'embedding_size': 300, 'n_heads': 2, 'n_layers': 2, 'ffn_size': 300, 'dropout': 0.1, 'attention_dropout': 0.0, 'relu_dropout': 0.1, 'learn_positional_embeddings': False, 'embeddings_scale': True, 'n_entity': 64368, 'n_relation': 214, 'n_concept': 29308, 'n_con_relation': 48, 'dim': 128, 'n_hop': 2, 'kge_weight': 1, 'l2_weight': 2.5e-06, 'n_memory': 32, 'item_update_mode': '0,1', 'using_all_hops': True, 'num_bases': 8}
160it [00:00, 10268.99it/s]
160it [00:00, 5124.10it/s]
160it [00:00, 341.41it/s]
160it [00:00, ?it/s]
C:\Users\thami\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
365028 46
  0%|                                                                                                                              | 0/403 [00:00<?, ?it/s]C:\Users\thami\Documents\GitHub\KGSF---movie-recommender-system\models\graph.py:137: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  attention = F.softmax(e)
C:\Users\thami\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
gen loss is 20.364559
 12%|██████████████▎                                                                                                    | 50/403 [10:25<1:19:09, 13.46s/it]gen loss is 4.826455
 25%|████████████████████████████▎                                                                                     | 100/403 [22:11<1:12:58, 14.45s/it]gen loss is 2.195465
 37%|███████████████████████████████████████████▏                                                                        | 150/403 [34:07<56:12, 13.33s/it]gen loss is 1.802645
 50%|█████████████████████████████████████████████████████████▌                                                          | 200/403 [44:54<43:45, 12.93s/it]gen loss is 1.483836
 62%|███████████████████████████████████████████████████████████████████████▉                                            | 250/403 [55:46<33:13, 13.03s/it]gen loss is 1.338942
 74%|████████████████████████████████████████████████████████████████████████████████████▊                             | 300/403 [1:07:37<24:41, 14.38s/it]gen loss is 1.152639
 87%|███████████████████████████████████████████████████████████████████████████████████████████████████               | 350/403 [1:20:18<13:34, 15.38s/it]gen loss is 1.006778
 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 400/403 [1:31:59<00:40, 13.36s/it]gen loss is 0.890677
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 403/403 [1:32:33<00:00, 13.78s/it]
80it [00:00, 5121.25it/s]
80it [00:00, 5120.23it/s]
80it [00:00, 341.37it/s]
80it [00:00, ?it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 102/102 [22:48<00:00, 13.42s/it]
C:\Users\thami\AppData\Local\Programs\Python\Python310\lib\site-packages\nltk\translate\bleu_score.py:552: UserWarning:
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
C:\Users\thami\AppData\Local\Programs\Python\Python310\lib\site-packages\nltk\translate\bleu_score.py:552: UserWarning:
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
C:\Users\thami\AppData\Local\Programs\Python\Python310\lib\site-packages\nltk\translate\bleu_score.py:552: UserWarning:
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
{'ppl': 0, 'dist1': 0.01667181228774313, 'dist2': 0.03735720901512812, 'dist3': 0.05588144489039827, 'dist4': 0.07131830811979006, 'bleu1': 0.00442057763053185, 'bleu2': 0.0008632193000817166, 'bleu3': 0.000353075968324344, 'bleu4': 0.00021308465021889408, 'count': 3239}
generator model saved once------------------------------------------------
  0%|                                                                                                                              | 0/403 [00:00<?, ?it/s]gen loss is 1.003194
 12%|██████████████▎                                                                                                    | 50/403 [13:02<1:40:34, 17.09s/it]gen loss is 0.530975
 25%|████████████████████████████▎                                                                                     | 100/403 [26:47<1:30:27, 17.91s/it]gen loss is 0.369809
 37%|██████████████████████████████████████████▍                                                                       | 150/403 [39:59<1:01:00, 14.47s/it]gen loss is 0.315573
 50%|█████████████████████████████████████████████████████████▌                                                          | 200/403 [53:38<54:37, 16.15s/it]gen loss is 0.296489
 62%|██████████████████████████████████████████████████████████████████████▋                                           | 250/403 [1:07:44<49:18, 19.33s/it]gen loss is 0.315420
 74%|████████████████████████████████████████████████████████████████████████████████████▊                             | 300/403 [1:21:44<28:44, 16.74s/it]gen loss is 0.299678
 87%|███████████████████████████████████████████████████████████████████████████████████████████████████               | 350/403 [1:35:41<14:09, 16.02s/it]gen loss is 0.300865
 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 400/403 [1:49:46<00:56, 18.75s/it]gen loss is 0.304575
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 403/403 [1:50:25<00:00, 16.44s/it]
80it [00:00, ?it/s]
80it [00:00, 5121.09it/s]
80it [00:00, 320.03it/s]
80it [00:00, ?it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 102/102 [25:10<00:00, 14.81s/it]
{'ppl': 0, 'dist1': 0.03334362457548626, 'dist2': 0.0691571472676752, 'dist3': 0.09817845013893177, 'dist4': 0.11762889780796543, 'bleu1': 0.004006475620043639, 'bleu2': 0.0008228242765893148, 'bleu3': 0.0003117744297684617, 'bleu4': 0.00020526065956567621, 'count': 3239}
  0%|                                                                                                                              | 0/403 [00:00<?, ?it/s]gen loss is 0.583034
 12%|██████████████▎                                                                                                    | 50/403 [14:07<1:34:18, 16.03s/it]gen loss is 0.127072
 25%|████████████████████████████▎                                                                                     | 100/403 [28:11<1:31:28, 18.11s/it]gen loss is 0.114109
 37%|██████████████████████████████████████████▍                                                                       | 150/403 [41:42<1:03:20, 15.02s/it]gen loss is 0.114305
 50%|████████████████████████████████████████████████████████▌                                                         | 200/403 [55:27<1:00:29, 17.88s/it]gen loss is 0.118052
 62%|██████████████████████████████████████████████████████████████████████▋                                           | 250/403 [1:09:17<42:23, 16.63s/it]gen loss is 0.137049
 74%|████████████████████████████████████████████████████████████████████████████████████▊                             | 300/403 [1:23:02<28:09, 16.41s/it]gen loss is 0.143921
 87%|███████████████████████████████████████████████████████████████████████████████████████████████████               | 350/403 [1:36:41<15:09, 17.16s/it]gen loss is 0.152653
 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 400/403 [1:50:27<00:46, 15.49s/it]gen loss is 0.173847
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 403/403 [1:51:03<00:00, 16.53s/it]
80it [00:00, 5121.25it/s]
80it [00:00, 5110.56it/s]
80it [00:00, 269.57it/s]
80it [00:00, ?it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 102/102 [25:17<00:00, 14.87s/it]
{'ppl': 0, 'dist1': 0.03426983636924977, 'dist2': 0.08274158690953998, 'dist3': 0.12009879592466811, 'dist4': 0.14387156529793146, 'bleu1': 0.004083165782045978, 'bleu2': 0.0010334599478503408, 'bleu3': 0.0003465641241075144, 'bleu4': 0.0001738212167343003, 'count': 3239}
80it [00:00, 5133.79it/s]
80it [00:00, 5119.92it/s]
80it [00:00, 284.51it/s]
80it [00:00, ?it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 102/102 [25:22<00:00, 14.93s/it]
{'ppl': 0, 'dist1': 0.03426983636924977, 'dist2': 0.08274158690953998, 'dist3': 0.12009879592466811, 'dist4': 0.14387156529793146, 'bleu1': 0.004083165782045978, 'bleu2': 0.0010334599478503408, 'bleu3': 0.0003465641241075144, 'bleu4': 0.0001738212167343003, 'count': 3239}
80it [00:00, ?it/s]
80it [00:00, 5120.16it/s]
80it [00:00, 300.88it/s]
80it [00:00, ?it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 102/102 [26:12<00:00, 15.42s/it]
{'ppl': 0, 'dist1': 0.03426983636924977, 'dist2': 0.08274158690953998, 'dist3': 0.12009879592466811, 'dist4': 0.14387156529793146, 'bleu1': 0.004083165782045978, 'bleu2': 0.0010334599478503408, 'bleu3': 0.0003465641241075144, 'bleu4': 0.0001738212167343003, 'count': 3239}